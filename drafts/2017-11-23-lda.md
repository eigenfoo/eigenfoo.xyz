---
layout: post
mathjax: true
title: Linear Discriminant Analysis
---

_Linear discriminant analysis_ (commonly abbreviated to LDA) is a very common
dimensionality reduction technique for classification problems. However, that's
something of an understatement: it does so much more than "just" dimensionality
reduction.

In plain English, if you have a lot of data (i.e. a large number of features)
from which you wish to classify observations, LDA will help you transform your
data so as to make the classes as distinct as possible. More rigorously, LDA
will find the linear projection of your data into a lower-dimensional subspace
that optimizes some measure of class separation. The dimension of this subspace
is necessarily strictly less than the number of classes.

This property makes LDA so effective that it's sometimes considered a
classification algorithm in itself: simply asdfasdf.

The goal of this post is to give a comprehensive explanation of LDA. As such,
LDA is explained three times:

1. LDA as an algorithm: what does it do, and how does it do it?
2. LDA as a theorem: a rigorous mathematical derivation of LDA
3. LDA as a machine learning technique: practical considerations when using LDA

## As an Algorithm

### The Problem

Before we dive into LDA, it's instructive to get an intuitive grasp of what LDA
attempts to accomplish.

Suppose that:

1. You have very high-dimensional data, and that
2. You are dealing with a classification problem

This could mean that the number of features is greater than the number of
observations, or it could mean that you suspect there are noisy features that
contain little information, or anything in between.

Given that this is the problem at hand, you wish to accomplish two things:

1. Reduce the number of features (i.e. reduce the dimensionality of your feature
   space), and
2. Preserve (or even increase!) the "distinguishability" of your classes or the 
   "separatedness" of the classes in your feature space.

This is the problem that LDA attempts to solve. It should be fairly obvious why
this problem might be worth solving.

To judiciously appropriate a term from signal processing, we are interested in
increasing the signal-to-noise ratio of our data, by both extracting or
synthesizing features that are useful in classifying our data (amplifying our
signal), and throwing out the features that are not as useful (attenuating our
noise).

Below is nice cartoon that may help our intuition about the problem: 

<img align="middle" width="300" height="300"
src="https://raw.githubusercontent.com/eigenfoo/eigenfoo.github.io/8a6a7a30/stuff/lda-pic.png">

A couple of points to make:
- LD1 and LD2 are among the projections that LDA would consider. In reality, LDA
  would consider _all possible_ projections, not just those along the x and y
  axes.
- LD1 is the one that LDA would actually come up with: this projection gives the
  best "separation" of the two classes.
- LD2 is a horrible projection by this metric: both classes get horribly
  overlapped... (For those who know what
  [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) is, this is
  the first principal component that PCA would give us! More on the relationship
  between LDA and PCA below).

### The Solution

First, some definitions:

Let:
- $$n$$ be the number of classes
- $$\mu$$ be the mean of all observations
- $$N_i$$ be the number of observations in the $$i$$th class
- $$\mu_i$$ be the mean of the $$i$$th class
- $$\Sigma_i$$ be the covariance matrices of the $$i$$th class

Now, define $$S_B$$ to be the _within-class scatter matrix_, given by

$$
\begin{align*}
    S_W = \sum_{i=1}^{n}{\Sigma_i}
\end{align*}
$$

and define $$S_W$$ to be the _between-class scatter matrix_, given by

$$
\begin{align*}
    S_W = \sum_{i=1}^{n}{N_i (\mu_i - \mu) (\mu_i - \mu)^T}
\end{align*}
$$

[Diagonalize](https://en.wikipedia.org/wiki/Diagonalizable_matrix) $$S_W^{-1}
S_B$$ to get its eigenvalues and eigenvectors.

Pick the $$k$$ largest eigenvalues, and their associated eigenvectors. We will
project our observations onto the subspace spanned by these vectors.

Concretely, what this means is that we must form the matrix $$A$$, whose rows
are the $$k$$ eigenvectors chosen above. $$W$$ will allow us to transform our
observations into the new subspace via the equation

$$ y = Ax $$

(where $$y$$ is our transformed observation, and $$x$$ is our original
observation).

And that's it!

For a more detailed and intuitive explanation of the LDA "recipe", see
[Sebastian Raschka's blog post on
LDA](http://sebastianraschka.com/Articles/2014_python_lda.html).

## As a Theorem

*Derivation:*

In order to quantify class separability, we need to formulate it as a number.
This number should be bigger when the between-class scatter is bigger, and
smaller when the within-class scatter is larger. There are many such numbers
that do this (in fact, [Fukunaga's _Introduction to Statistical Pattern
Recognition_](https://www.elsevier.com/books/introduction-to-statistical-pattern-recognition/fukunaga/978-0-08-047865-4)
considers no less than four!) Here, we will concern ourselves with just one:

$$ J_1 = tr(S_W^{-1} S_B) $$


There's one more quirk of LDA that is very much worth knowing. Suppose you have
1000 observations, 200 features and 10 classes, and you run LDA. It turns out
that the _maximum_ number of features LDA can give you is one less than the
number of class, so in this case, 9! Here's why:

*Proposition:* $$S_W^{-1} S_B$$ has at most $$n-1$$ non-zero eigenvalues. (This
implies that LDA is capable of reducing the dimension to at least $$n-1$$!)

*Proof:* First, we need a lemma.

*Lemma:* Suppose $${v_i}_{i=1}^{n}$$ is a set of linearly dependent vectors, and
let $$\alpha_i$$ be $$n$$ coefficients.  Then, $$M = \sum_{i=1}^{n}{\alpha_i v_i
v_i^{T}}$$, a linear combination of outer products of the vectors with
themselves, is rank deficient.

INSERT PROOF HERE

With the lemma, we're now ready to prove the theorem.

We have that 

$$
\begin{align*}
\frac{1}{n} \sum_{i=1}^{n}{\mu_i} = \mu \implies \sum_{i=1}^{n}{\mu_i-\mu} = 0
\end{align*}
$$

So $${\mu_i-\mu}_{i=1}^{n}$$ is a linearly dependent set. Applying our lemma, we
see that

$$ S_B = \sum_{i=1}^{n}{N_i (\mu_i-\mu)(\mu_i-\mu)^{T}} $$ 

must be rank deficient. Thus, $$rank(B_W) \leq n-1$$. Now, $$rank(AB) \leq
rank(A)rank(B)$$, so 

$$
\begin{align*}
rank(S_W^{-1}S_B) \leq min{rank(S_W^{-1}, rank(S_B)}
\leq n-1
\end{align*}
$$

as desired.

## As a Machine Learning Technique

That was a quite a bit of math, but how is LDA actually used in practice? One of
the easiest ways is to jump straight into an implementation of LDA.
`scikit-learn` has [a very well-documented implementation of
LDA](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis):
reading the docs is a great way to learn!

Below are a few miscellaneous comments on practical considerations when using
LDA. 

### Regularization (a.k.a. Shrinkage)

`scikit-learn`'s implementation of LDA has an interesting optional parameter:
`shrinkage`. What's that about?

[Here's a wonderful Cross Validated
post](https://stats.stackexchange.com/questions/106121/does-it-make-sense-to-combine-pca-and-lda/109810#109810)
on how LDA can introduce overfitting. In essence, matrix inversion is a highly
sensitive operation (in that small changes in the matrix may lead to large
changes in its inverse, so that even a tiny bit of noise will be amplified upon
inverting the matrix), and so unless the estimate of the within-class scatter
matrix $$S_W$$ is very good, its inversion is likely to introduce overfitting.

One way to combat that is through regularizing LDA. It basically replaces
$$S_W$$ with $$(1-t)S_W + tI$$, where $$I$$ is the identity matrix, and $$t$$ is
the _regularization parameter_, or the _shrinkage constant_. That's what
`scikit`'s `shrinkage` parameter is: it's $$t$$.

If you're interested in _why_ this linear combination of the within-class
scatter and the identity give such a well-conditioned estimate of $$S_W$$, check
out [the original paper by Ledoit and
Wolf](https://www.sciencedirect.com/science/article/pii/S0047259X03000964).
Their original motivation was in financial portfolio optimization, so they've
also authored several other papers
([here](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=433840&rec=1&srcabs=290916&alg=7&pos=6)
and [here](https://www.sciencedirect.com/science/article/pii/S0927539803000070))
that go into the more financial details. That needn't concern us though:
covariance matrices are ubiquitous in any data-driven field.

For an illustration, `amoeba`'s post on Cross Validated gives a good example of
LDA overfitting, and how regularization can help combat that.

### Close relatives: PCA and QDA

PCA tries to find the axes with _maximum variance_ for the whole data set,
whereas LDA tries to find the axes for best _class separability_.

### LDA as a classifier

It basically is, but some people find that normality assumptions are needed for
classification, while dimensionality reduction doesn't really require that...

