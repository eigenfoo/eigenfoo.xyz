---
layout: post
mathjax: true
title: Linear Discriminant Analysis
---

_Linear discriminant analysis_ (commonly abbreviated to LDA) is a very commonly
used dimensionality reduction technique for classification. However, saying that
does LDA an injustice: it does so much more than "just" dimensionality
reduction.

In plain English, if you have a lot of data (i.e. a large number of features)
from which you wish to classify observations, LDA will help you transform your
data so as to make the classes as distinct as possible. More rigorously, LDA
will find the linear projection of your data into a lower-dimensional subspace
that optimizes some measure of class separation. The dimension of this subspace
is necessarily strictly less than the number of classes.

This property makes LDA so effective that it's sometimes considered a
classification algorithm in itself: simply asdfasdf.

The goal of this post is to give a comprehensive explanation of LDA. As such,
LDA is explained three times:

1. LDA as an algorithm: what does it do, and how does it do it?
2. LDA as a theorem: a rigorous mathematical derivation of LDA
3. LDA as a machine learning technique: practical considerations when using LDA

## As an Algorithm

### The Problem

Before we dive into LDA, it's instructive to get an intuitive grasp of what LDA
attempts to accomplish.

Suppose that:

1. You have very high-dimensional data, and that
2. You have a classification problem at hand

This could mean that the number of features is greater than the
number of observations, or it could mean that you suspect there are [features
that contain little information](url to noise feature), or anything in between.

You wish to accomplish two things:

1. Reduce the number of features (i.e. reduce the dimensionality of your feature
   space), and
2. Preserve (or even increase!) the "distinguishability" of your classes or the 
   "separatedness" of the classes in your feature space.

This is the problem that LDA attempts to solve. It should be fairly obvious why
this problem might be worth solving.

To judiciously appropriate a term from signal processing, we are interested in
increasing the signal-to-noise ratio of our data, by both extracting or
synthesizing features that are useful in classifying our data (amplifying our
signal), and throwing out the features that are not as useful (attenuating our
noise).

Below is nice cartoon that may help our intuition about the problem: 

<img align="middle" src="https://raw.githubusercontent.com/eigenfoo/eigenfoo.github.io/8a6a7a30/stuff/lda-pic.png">

A couple of points to make:
- LD1 and LD2 are among the projections that LDA would consider. In reality, LDA
  would consider _all possible_ projections, not just those along the x and y
  axes.
- LD1 is the one that LDA would actually come up with: this projection gives the
  best "separation" of the two classes.
- LD2 is a horrible projection by this metric: both classes get horribly
  overlapped... (For those who know what
  [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) is, this is
  the projection that PCA would give us! More on the relationship between LDA
  and PCA below).

### The Solution

First, some definitions:

Let:
- $$n$$ be the number of classes
- $$\mu$$ be the mean of all observations
- $$N_i$$ be the number of observations in the $$i$$th class
- $$\mu_i$$ be the mean of the $$i$$th class
- $$\Sigma_i$$ be the covariance matrices of the $$i$$th class

Now, define $$S_B$$ to be the _within-class scatter matrix_, given by

$$
\begin{align*}
    S_W = \sum_{i=1}^{n}{\Sigma_i}
\end{align*}
$$

and define $$S_W$$ to be the _between-class scatter matrix_, given by

$$
\begin{align*}
    S_W = \sum_{i=1}^{n}{N_i (\mu_i - \mu) (\mu_i - \mu)^T}
\end{align*}
$$

## As a Theorem

Fukunaga 1990
Bishop PRML

## Practical Considerations

- Regularization
https://stats.stackexchange.com/questions/106121/does-it-make-sense-to-combine-pca-and-lda

- How LDA is a "classifier"

- Differences between LDA and PCA!

- PCA and PLS as preprocessing to LDA...