---
mathjax: true
title: ~~Fruit~~ Loops and Learning 2 - Implementing SVM+
excerpt: "A neat trick to implement LUPI and SVM+"
categories:
    - Mathematics
    - Machine Learning
tags:
    - lupi
    - svm+
header:
  teaser: /assets/images/loops.jpeg
  overlay_image: /assets/images/loops.jpeg
  caption: "Photo credit: [wallpapercraze.com](http://wallpapercraze.com/images/wallpapers/fruitloops-441535.jpeg)"
  last_modified_at: 2018-02-25
---

In my last post, I introduced the LUPI paradigm and described one algorithm that
interoperates with that paradigm: SVM+. I described LUPI as a up-and-coming
perspective of machine learning that had great potential.

In this short post, I'll describe the results of [one
paper](https://www.researchgate.net/publication/301880839_Simple_and_Efficient_Learning_using_Privileged_Information)
that shows how to implement SVM+ in a simple and efficient way. With this
computational trick, the authors rechristen the algorithm as SVM2+.

---

## What's wrong with SVM+?

_It has $$n$$ training examples but $$2n$$ variables to estimate._ This is twice as
many variables to estimate as the standard formulation of the [vanilla
SVM](https://en.wikipedia.org/wiki/Support_vector_machine#Computing_the_SVM_classifier).
This problem is inherent to the Lagrangian dual formulation that Vapnik and
Vashist proposed in 1995.

Even worse, the optimization problem has constraints that are very different
from those of the standard SVM. In essence, this means that efficient libraries
out-of-the-box solvers for the standard SVM (e.g.
[LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) and
[LIBLINEAR](https://www.csie.ntu.edu.tw/~cjlin/liblinear/)) can't be used to
train an SVM+ model. 

## The Solution

Essentially, instead of using the hinge loss when training SVM+, we will instead
use the _squared_ hinge loss. It turns out that changing the loss function in
this way leads to a tiny miracle.

The formulation of SVM+ becomes identical to that of the standard SVM, except we
replace the Gram matrix (a.k.a. kernel matrix) $$\bf K$$ by $$\bf K  + \bf
Q_\lambda  \odot (\bf y y^t)$$ where

- $$\bf y $$ is the target vector
- $$\odot$$ denotes the Hadamard product
- $$\bf{Q_\lambda}$$ is given by $$ Q_\lambda = \frac{1}{\lambda} (\tilde{K}
  (\frac{\lambda}{C} I_n + \tilde{K})^{-1} \tilde{K}) $$, and
- $$\bf \tilde{K}$$ is the Gram matrix formed by the privileged information

The proof of this proposition is easy enough to follow, but hardly the point of
the paper: the interested reader can find it in the paper.

The main thing to keep in mind is that by replacing the hinge loss with the
squared hinge loss, the SVM+ formulation can now be solved with existing
libraries!
