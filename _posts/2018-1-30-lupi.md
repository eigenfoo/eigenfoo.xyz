---
mathjax: true
title: Loopy (LUPI) Learning
excerpt: "The LUPI (learning using privileged information) paradigm, and why
it's exciting"
header:
  teaser: /assets/images/lda-pic.png
  last_modified_at: 2018-01-29
---

Here's a short story you might know: you have a black box called the _Machine
Learning Algorithm_. It's got two modes: training mode and testing mode.  You
set it to training mode, and throw in a lot (sometimes _a lot_ a lot) of ordered
pairs $$(x_i, y_i), 1 \leq i \leq l $$. Here, the $$x_i$$ are called the
_examples_ and the $$y_i$$ are called the _targets_. Then, you set it to testing
mode and throw in some more $$x_i$$s, for which you don't have the corresponding
$$y_i$$s. You hope the $$y_i$$s that come out are in some sense the "right"
ones.

Generally speaking, this is the parable of supervised learning. However, Vapnik
(inventor of the [SVM](https://en.wikipedia.org/wiki/Support_vector_machine))
recently described a new way to think about machine learning
([here](http://www.engr.uconn.edu/~jinbo/doc/vladimir_newparadiam.pdf) and
[here](http://jmlr.csail.mit.edu/papers/volume16/vapnik15b/vapnik15b.pdf)):
learning using privileged information, or LUPI.

This blog post is meant to introduce the LUPI paradigm of machine learning to
those who are generally familiar with supervised learning and SVMs (including
their mathematical foundations) and are interested in seeing it extended to the
LUPI paradigm.

The main idea is that instead of two-tuples $$(x_i, y_i)$$, the machine is fed
three-tuples $$(x_i, x_i^{*}, y_i) $$, where x^{*} is the so-called _privileged
information_ that is only available during training, and not during testing. The
hope is that this information will train the model to better generalize during
the testing phase.

Vapnik and Vashist offer many examples in which LUPI can be applied in real
life: in bioinformatics and proteomics (where advanced biological models, which
the machine might not necessarily "understand", serve as the privileged
information), in financial time series analysis (where future movements of the
time series are the unknown at prediction time, but are available
retrospectively), and in the classic MNIST dataset, where the images were
converted to a lower resolution, but each annotated with a "poetic description"
(which was available for the training data but not for the testing data).

Vapnik's team ran tests on well-known datasets in all three application areas
and found that his newly-developed LUPI methods performed noticeably better than
traditional SVMs in both convergence time and estimation of the proper function.
In fact, Vapnik's proof-of-concept experiments are so whacky that they actually
[make for an entertaining read
](https://nautil.us/issue/6/secret-codes/teaching-me-softly)!

## Classical SVMs (separable and non-separable case)

There are many ways of thinking about SVMs, but I think that the one that is
most instructive here is to think of them as solving the following optimization
problem:

> Minimize $$ \frac{1}{2} \norm{w}^2 $$
>
> subject to $$ y_i [ w \dot x_i + b ] \geq 1, 1 \leq i \leq l $$.

Effectively, this finds the parameters ($$w$$ and $$b$$) of the maximum
margin hyperplane, with $$l_2$$ regularization.

In the non-separable case, we concede that our hyperplane may not classify all
examples perfectly, and introduce so-called _slack variables_, $$ \xi_i \geq 0 $$,
which measure the misclassification of example $$i$$. With that, the
optimization becomes:

> Minimize $$ \frac{1}{2} \norm{w}^2 + C\sum_{i=1}^{l}{\xi_i} $$
>
> subject to $$ y_i [ w \dot x_i + b ] \geq 1 - \xi_i, 1 \leq i \leq l $$.

where $$ C $$ is some regularization parameter.

In both cases, the decision rule is then simply
$$ \hat{y} = \text{sign}(w \dot x + b) $$.

An important thing to note is that, in the separable case, the SVM uses $$l$$
examples to estimate the $$n$$ components of $$w$$, whereas in the nonseparable
case, the SVM uses $$l$$ examples to estimate $$n+l$$ parameters: the $$n$$
components of $$w$$ and $$l$$ values of slacks $$\xi_i$$.  Thus, in the
non-separable case, the number of parameters to be estimated is always larger
than the number of examples: it does not matter here that most of slacks may be
equal to zero: the SVM still has to estimate all of them.

The way the optimization problems are actually _solved_ is fairly involved (they
require Lagrange multipliers), but in terms of getting an intuitive feel for how
SVMs work, examining the optimization problems will suffice for our needs!

## The SVM+

In his paper introducing the LUPI paradigm, Vapnik outlines the _SVM+_, a
modified form of the SVM that fits well into the LUPI paradigm, using privileged
information to improve performance.

The innovation of the SVM+ algorithm is that is uses the privileged information
to estimate the slack variables. Given the training three-tuple $$ (x, x^{*}, y)
$$, we map $$x$$ to the feature space $$Z$$, and $$x^{*}$$ to the privileged
space $$Z^{*}$$. Then, the decision rule is $$ \hat{y} = \text{sign}(w \dot x +
b) $$ and the slack variables are estimated by $$ \xi = w^{*} \dot x^{*} + b^{*}
$$.

In order to find $$w$$, $$b$$, $$w^{*}$$ and $$b^{*}$$, we solve the following
optimization problem:

> Minimize $$ \frac{1}{2} ( \norm{w}^2 + \gamma \norm{w^{*}}^2 ) +
> C \sum{i=1}{l}{w^{*} \dot x_i^{*} + b^{*}} $$
> 
> subject to $$ y_i [ w \dot x_i + b ] \geq 1 - (w^{*} \dot x^{*} + b^{*}),
> (w^{*} \dot x^{*} + b^{*}) \geq 0, 1 \leq i \leq l $$.

Again, the method of actually solving this optimization problem involves
Lagrange multipliers and quadratic programming, but I think the intuition is
captured in the problem statement.

## Interpretation of the SVM+

The SVM+ has a very ready interpretation. Instead of a single feature space, it
has two: one in which the non-privileged information lives (where decisions are
made), and one in which the privileged information lives (where slack variables
are estimated).

Of course, SVMs is a technique with many possible interpretations, of which my
presentation (in terms of the optimization of $$w$$ and $$b$$) is just one. For
example, it's possible to think of SVMs in terms of kernels functions, or as
linear classifiers minimizing hinge loss. In all cases, it's possible and
worthwhile to understand that interpretation of SVMs, and how the LUPI paradigm
contributes to or extends that interpretation. I'm hoping to write a piece later
to explain these exact topics.

Vapnik also puts a great emphasis on analyzing the SVM+ based on its statistical
learning theoretic properties (in particular, analyzing its rate of convergence
via the [VC dimension](https://en.wikipedia.org/wiki/VC_dimension)). Vapnik has
written an [entire
book](https://www.amazon.com/Statistical-Learning-Theory-Vladimir-Vapnik/dp/0471030031)
on this stuff (which I haven't read), so I'll leave that part aside for now.

## Extensions to the SVM+

In his paper, Vapnik makes it clear that LUPI is a very general and abstract
paradigm, and as such there is plenty of room for creativity and innovation -
not just in researching and developing new LUPI methods and algorithms, but also
in implementing and applying them. It is unknown how to best go about supplying
privileged information so as to get good performance. How should the data be
feature engineered? How much signal should be in the privileged information?
These are all open questions.

Vapnik himself opens up three avenues to extend the SVM+ algorithm:

1. _a mixture model of slacks:_ when slacks are estimated by a mixture of a
   smooth function and some prior
2. _a model where privileged information is available only for a part of the
   training data:_ where we can only supply privileged information on a small
   subset of the training examples
3. _multiple-space privileged information:_ where the privileged information we
   can supply do not all share the same features



**I'd love to hear any feedback on this post! Send me a tweet or an email.**
